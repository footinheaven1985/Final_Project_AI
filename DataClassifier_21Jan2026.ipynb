{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6rk1+EIHxxwwm2DZlQDBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/footinheaven1985/Final_Project_AI/blob/main/DataClassifier_21Jan2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Colab Gradio App:\n",
        "# 1) Upload + index reference docs into Pinecone (dim=1536)\n",
        "# 2) Upload a new doc\n",
        "# 3) Similarity search + classification + CITATIONS (top matches)\n",
        "# ============================================\n",
        "\n",
        "!pip -q install \"pinecone-client>=3.0.0\" openai pypdf python-docx gradio pandas\n",
        "\n",
        "import os, re, uuid, time\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from google.colab import userdata # Import userdata for secrets\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from pypdf import PdfReader\n",
        "import docx\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "# IMPORTANT: Replace \"PASTE_PINECONE_KEY\" with your actual Pinecone API key.\n",
        "# For better security, store it in Colab Secrets and use userdata.get(\"PINECONE_API_KEY\")\n",
        "PINECONE_API_KEY = userdata.get(\"PINECONE_API_KEY\") or \"PASTE_PINECONE_KEY\"\n",
        "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\") or \"your-index-name\"\n",
        "\n",
        "# IMPORTANT: Replace \"PASTE_OPENAI_KEY\" with your actual OpenAI API key.\n",
        "# For better security, store it in Colab Secrets and use userdata.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") or \"PASTE_OPENAI_KEY\"\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-small\"  # 1536 dims ‚úÖ\n",
        "INDEX_DIMENSION = 1536\n",
        "METRIC = \"cosine\"\n",
        "\n",
        "LABEL_KEY = \"label\"      # metadata label field\n",
        "TOP_K = 5                # nearest neighbors per chunk query\n",
        "\n",
        "# Chunking (character-based; easy + works decently)\n",
        "CHUNK_SIZE = 1200\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# Default namespace for runs\n",
        "DEFAULT_NAMESPACE = \"doc_classification\"\n",
        "# --------------------------------------\n",
        "\n",
        "\n",
        "# ============== Text extraction ==============\n",
        "def extract_text_from_pdf(path: str) -> str:\n",
        "    reader = PdfReader(path)\n",
        "    pages = [(p.extract_text() or \"\") for p in reader.pages]\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def extract_text_from_txt(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def extract_text_from_docx(path: str) -> str:\n",
        "    d = docx.Document(path)\n",
        "    return \"\\n\".join(p.text for p in d.paragraphs)\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def load_file_as_text(path: str) -> str:\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == \".pdf\":\n",
        "        text = extract_text_from_pdf(path)\n",
        "    elif ext == \".txt\":\n",
        "        text = extract_text_from_txt(path)\n",
        "    elif ext == \".docx\":\n",
        "        text = extract_text_from_docx(path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}. Use PDF/TXT/DOCX.\")\n",
        "    return normalize_text(text)\n",
        "\n",
        "\n",
        "# ============== Chunking ==============\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(len(text), start + chunk_size)\n",
        "        chunks.append(text[start:end])\n",
        "        if end == len(text):\n",
        "            break\n",
        "        start = max(0, end - overlap)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ============== Embeddings ==============\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
        "    return [d.embedding for d in resp.data]\n",
        "\n",
        "\n",
        "# ============== Pinecone: ensure index ==============\n",
        "def ensure_pinecone_index():\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    existing = [i[\"name\"] for i in pc.list_indexes()]\n",
        "\n",
        "    if PINECONE_INDEX_NAME not in existing:\n",
        "        pc.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=INDEX_DIMENSION,\n",
        "            metric=METRIC,\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "        )\n",
        "        # wait until ready\n",
        "        while True:\n",
        "            desc = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "            if desc.status.get(\"ready\"):\n",
        "                break\n",
        "            time.sleep(2)\n",
        "    else:\n",
        "        desc = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "        if desc.dimension != INDEX_DIMENSION:\n",
        "            raise ValueError(f\"Index dimension mismatch: index={desc.dimension} vs expected={INDEX_DIMENSION}\")\n",
        "\n",
        "    return pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "\n",
        "# ============== Indexing ==============\n",
        "def parse_label_map(label_map_text: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Expects lines like:\n",
        "      file1.pdf,Invoice\n",
        "      contract.docx,Contract\n",
        "    Filenames must match the uploaded files' base names.\n",
        "    \"\"\"\n",
        "    mapping = {}\n",
        "    for line in (label_map_text or \"\").splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if \",\" not in line:\n",
        "            raise ValueError(f\"Bad line (missing comma): {line}\")\n",
        "        fname, label = [x.strip() for x in line.split(\",\", 1)]\n",
        "        if not fname or not label:\n",
        "            raise ValueError(f\"Bad line (empty filename or label): {line}\")\n",
        "        mapping[fname] = label\n",
        "    return mapping\n",
        "\n",
        "def upsert_reference_files(index, files_list: List[str], label_map: Dict[str, str], namespace: str) -> Tuple[int, List[str]]:\n",
        "    \"\"\"\n",
        "    Upserts chunks of each file with metadata including label and citations fields.\n",
        "    Returns: (num_vectors, warnings)\n",
        "    \"\"\"\n",
        "    warnings = []\n",
        "    to_upsert = []\n",
        "\n",
        "    for path in files_list:\n",
        "        base = os.path.basename(path)\n",
        "        if base not in label_map:\n",
        "            warnings.append(f\"‚ö†Ô∏è No label provided for '{base}' ‚Äî skipped.\")\n",
        "            continue\n",
        "\n",
        "        label = label_map[base]\n",
        "        text = load_file_as_text(path)\n",
        "        if len(text) < 30:\n",
        "            warnings.append(f\"‚ö†Ô∏è Very little text in '{base}' ‚Äî skipped.\")\n",
        "            continue\n",
        "\n",
        "        chunks = chunk_text(text)\n",
        "        vecs = embed_texts(chunks)\n",
        "\n",
        "        for i, (chunk, vec) in enumerate(zip(chunks, vecs)):\n",
        "            vec_id = f\"{base}::{uuid.uuid4().hex[:10]}::{i}\"\n",
        "            md = {\n",
        "                LABEL_KEY: label,\n",
        "                \"source_file\": base,\n",
        "                \"chunk_id\": i,\n",
        "                \"text_preview\": chunk[:300],  # used for citations\n",
        "            }\n",
        "            to_upsert.append((vec_id, vec, md))\n",
        "\n",
        "    # Batch upsert\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(to_upsert), batch_size):\n",
        "        index.upsert(vectors=to_upsert[i:i+batch_size], namespace=namespace)\n",
        "\n",
        "    return len(to_upsert), warnings\n",
        "\n",
        "\n",
        "# ============== Similarity search + classification + citations ==============\n",
        "def query_similar(index, vec: List[float], top_k: int, namespace: str) -> List[dict]:\n",
        "    res = index.query(vector=vec, top_k=top_k, include_metadata=True, namespace=namespace)\n",
        "    return res.get(\"matches\", [])\n",
        "\n",
        "def classify_and_cite(matches: List[dict], label_key: str = LABEL_KEY) -> Tuple[str, float, pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Returns: predicted_label, confidence, citations_df, citations_markdown\n",
        "    Confidence = weighted vote share by similarity score.\n",
        "    Citations = top matches with score + preview + source file + chunk_id.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    weighted = defaultdict(float)\n",
        "\n",
        "    for m in matches:\n",
        "        md = m.get(\"metadata\") or {}\n",
        "        label = md.get(label_key)\n",
        "        score = float(m.get(\"score\", 0.0))\n",
        "        if not label:\n",
        "            continue\n",
        "        weighted[label] += score\n",
        "        rows.append({\n",
        "            \"score\": score,\n",
        "            \"label\": label,\n",
        "            \"source_file\": md.get(\"source_file\"),\n",
        "            \"chunk_id\": md.get(\"chunk_id\"),\n",
        "            \"text_preview\": md.get(\"text_preview\", \"\")\n",
        "        })\n",
        "\n",
        "    if not rows:\n",
        "        empty_df = pd.DataFrame(columns=[\"score\", \"label\", \"source_file\", \"chunk_id\", \"text_preview\"])\n",
        "        return (None, 0.0, empty_df, \"No labeled matches found. Make sure your indexed vectors include metadata labels.\")\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    winner = max(weighted.items(), key=lambda x: x[1])[0]\n",
        "    total = sum(weighted.values()) or 1.0\n",
        "    confidence = weighted[winner] / total\n",
        "\n",
        "    # Build a readable citation section (top 8 rows)\n",
        "    topn = min(8, len(df))\n",
        "    cite_lines = [f\"### Citations (Top {topn} matches)\"]\n",
        "    for i in range(topn):\n",
        "        r = df.iloc[i]\n",
        "        preview = (r[\"text_preview\"] or \"\").replace(\"\\n\", \" \")\n",
        "        if len(preview) > 220:\n",
        "            preview = preview[:220] + \"‚Ä¶\"\n",
        "        cite_lines.append(\n",
        "            f\"**{i+1}.** score={r['score']:.4f} ‚Ä¢ **{r['label']}** ‚Ä¢ `{r['source_file']}` (chunk {r['chunk_id']})\\n\\n> {preview}\\n\"\n",
        "        )\n",
        "\n",
        "    # Also show vote breakdown\n",
        "    breakdown = sorted(weighted.items(), key=lambda x: x[1], reverse=True)\n",
        "    breakdown_md = \"\\n\".join([f\"- **{lbl}**: {w:.4f}\" for lbl, w in breakdown])\n",
        "    cite_lines.append(\"### Vote breakdown (sum of similarity scores)\")\n",
        "    cite_lines.append(breakdown_md)\n",
        "\n",
        "    return winner, confidence, df, \"\\n\".join(cite_lines)\n",
        "\n",
        "\n",
        "# ============== Gradio actions ==============\n",
        "INDEX = ensure_pinecone_index()\n",
        "\n",
        "def index_reference_docs(ref_files, label_map_text, namespace):\n",
        "    if not namespace:\n",
        "        namespace = DEFAULT_NAMESPACE\n",
        "\n",
        "    if not ref_files:\n",
        "        return \"‚ùå Please upload at least one reference file.\", None\n",
        "\n",
        "    # Gradio File objects can be dict-like or have .name depending on version\n",
        "    paths = []\n",
        "    for f in ref_files:\n",
        "        if isinstance(f, str):\n",
        "            paths.append(f)\n",
        "        else:\n",
        "            # gradio typically provides a tempfile path at f.name\n",
        "            paths.append(getattr(f, \"name\", None) or f.get(\"name\"))\n",
        "\n",
        "    label_map = parse_label_map(label_map_text)\n",
        "    num_vecs, warnings = upsert_reference_files(INDEX, paths, label_map, namespace)\n",
        "\n",
        "    msg = [f\"‚úÖ Indexed **{num_vecs}** vectors into Pinecone.\", f\"**Index:** `{PINECONE_INDEX_NAME}`\", f\"**Namespace:** `{namespace}`\"]\n",
        "    if warnings:\n",
        "        msg.append(\"\\n\".join(warnings))\n",
        "    return \"\\n\\n\".join(msg), pd.DataFrame({\"uploaded_files\": [os.path.basename(p) for p in paths]})\n",
        "\n",
        "\n",
        "def classify_uploaded_doc(query_file, namespace, top_k):\n",
        "    if not namespace:\n",
        "        namespace = DEFAULT_NAMESPACE\n",
        "    if not query_file:\n",
        "        return \"‚ùå Please upload a file to classify.\", \"\", None\n",
        "\n",
        "    path = query_file if isinstance(query_file, str) else getattr(query_file, \"name\", None) or query_file.get(\"name\")\n",
        "\n",
        "    text = load_file_as_text(path)\n",
        "    if len(text) < 30:\n",
        "        return \"‚ö†Ô∏è Extracted text is very short; classification may be unreliable.\", \"\", None\n",
        "\n",
        "    chunks = chunk_text(text)\n",
        "    vecs = embed_texts(chunks)\n",
        "\n",
        "    all_matches = []\n",
        "    for v in vecs:\n",
        "        all_matches.extend(query_similar(INDEX, v, top_k=int(top_k), namespace=namespace))\n",
        "\n",
        "    pred, conf, cite_df, cite_md = classify_and_cite(all_matches, LABEL_KEY)\n",
        "\n",
        "    if pred is None:\n",
        "        headline = \"‚ùå Could not classify (no labeled matches found).\"\n",
        "    else:\n",
        "        headline = f\"‚úÖ **Predicted label:** {pred}\\n\\n**Confidence:** {conf:.2f}\"\n",
        "\n",
        "    return headline, cite_md, cite_df\n",
        "\n",
        "\n",
        "# ============== Gradio UI ==============\n",
        "with gr.Blocks(title=\"Pinecone Similarity Document Classifier (with Citations)\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "# Pinecone Similarity Document Classifier üîéüìÑ\n",
        "\n",
        "This app lets you:\n",
        "1) Upload **reference documents** (already labeled) ‚Üí **index** them into Pinecone\n",
        "2) Upload a **new document** ‚Üí **similarity search** ‚Üí get predicted **label** + **citations**\n",
        "\n",
        "**How to provide labels for reference docs:**\n",
        "In the box, add one line per file:\n",
        "`filename.pdf,Invoice`\n",
        "`contract.docx,Contract`\n",
        "\n",
        "Supported: **PDF / TXT / DOCX**\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"1) Index Reference Docs\"):\n",
        "        namespace_in = gr.Textbox(value=DEFAULT_NAMESPACE, label=\"Pinecone Namespace (optional)\")\n",
        "        ref_files = gr.File(file_count=\"multiple\", label=\"Upload reference docs (PDF/TXT/DOCX)\")\n",
        "        label_map = gr.Textbox(\n",
        "            label=\"Labels mapping (one per line: filename,label)\",\n",
        "            lines=6,\n",
        "            placeholder=\"example:\\ninvoice1.pdf,Invoice\\ninvoice2.pdf,Invoice\\nnda.docx,Contract\"\n",
        "        )\n",
        "        index_btn = gr.Button(\"Index Reference Docs\")\n",
        "        index_status = gr.Markdown()\n",
        "        indexed_files_df = gr.Dataframe(label=\"Uploaded Reference Files (for your confirmation)\", interactive=False)\n",
        "\n",
        "        index_btn.click(\n",
        "            fn=index_reference_docs,\n",
        "            inputs=[ref_files, label_map, namespace_in],\n",
        "            outputs=[index_status, indexed_files_df]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"2) Classify New Doc (with citations)\"):\n",
        "        namespace_in2 = gr.Textbox(value=DEFAULT_NAMESPACE, label=\"Pinecone Namespace (must match indexing)\")\n",
        "        topk_in = gr.Slider(1, 20, value=TOP_K, step=1, label=\"Top K neighbors per chunk\")\n",
        "        query_file = gr.File(file_count=\"single\", label=\"Upload file to classify (PDF/TXT/DOCX)\")\n",
        "        classify_btn = gr.Button(\"Classify\")\n",
        "        result_md = gr.Markdown()\n",
        "        citations_md = gr.Markdown()\n",
        "        citations_df = gr.Dataframe(label=\"Citations Table (sorted by score)\", interactive=False)\n",
        "\n",
        "        classify_btn.click(\n",
        "            fn=classify_uploaded_doc,\n",
        "            inputs=[query_file, namespace_in2, topk_in],\n",
        "            outputs=[result_md, citations_md, citations_df]\n",
        "        )\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "GFtvNzVhjJHZ",
        "outputId": "d8f4cf60-d991-4840-82f0-196f3e2c2afc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a788f9e234d8eb4b4b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a788f9e234d8eb4b4b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}